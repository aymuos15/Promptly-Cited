{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from cerebras.cloud.sdk import Cerebras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tex(text):\n",
    "    # Basic preprocessing to remove LaTeX commands, but keep section headers\n",
    "    text = re.sub(r'\\\\(?!section|subsection|subsubsection)([a-zA-Z]+)(\\[.*?\\])?({.*?})?', '', text)\n",
    "    return text\n",
    "\n",
    "#? getting all the tex files from every subdirectory\n",
    "def get_all_tex_files(directory):\n",
    "    tex_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.tex'):\n",
    "                tex_files.append(os.path.join(root, file))\n",
    "    return tex_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompt context for first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'dataset' #? Where all you extracted zip of every paper is.\n",
    "tex_files = get_all_tex_files(base_dir) #? Getting all the tex files from the directory\n",
    "\n",
    "# Process files\n",
    "papers = {}\n",
    "for file in tex_files: # Going through one folder at a time\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "    preprocessed_content = preprocess_tex(content) # Gets rid of all the latex commands\n",
    "\n",
    "    # remove 1000 characters from the end of the content and the start of the content\n",
    "    preprocessed_content = preprocessed_content[500:-500]\n",
    "\n",
    "    paper_name = os.path.relpath(file, base_dir).split(os.sep)[0] #? Getting the name of the paper from the folder name\n",
    "\n",
    "    # paper name here actually has all the subdirectories as well, so we remove them by omitting all which do not have the exact paper name.    \n",
    "    if paper_name not in papers:\n",
    "        papers[paper_name] = \"\"\n",
    "    papers[paper_name] += preprocessed_content + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paper_prompt(paper_title, paper_content, analysis_questions):\n",
    "    questions_str = \"\\n\".join(f\"- {q}\" for q in analysis_questions)\n",
    "    return f\"\"\"\n",
    "======== START OF PAPER: {paper_title} ========\n",
    "\n",
    "{paper_content}\n",
    "\n",
    "======== END OF PAPER: {paper_title} ========\n",
    "\n",
    "Please analyze this paper and answer the following questions:\n",
    "\n",
    "{questions_str}\n",
    "\n",
    "Base your analysis ONLY on the provided context. If you can't find information for a question, respond with \"Not available\".\n",
    "\"\"\"\n",
    "\n",
    "def parse_paper_analysis(response):\n",
    "    lines = response.split('\\n')\n",
    "    analysis = {}\n",
    "    current_key = None\n",
    "    for line in lines:\n",
    "        if line.strip().startswith('1.') or line.strip().startswith('2.'):\n",
    "            current_key = line.strip()\n",
    "            analysis[current_key] = \"\"\n",
    "        elif current_key and line.strip():\n",
    "            analysis[current_key] += ' ' + line.strip()\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 1:\n",
    "\n",
    "Inference on each paper indiviually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Cerebras client\n",
    "client = Cerebras(api_key='csk-83cjr2kjp5dke2f2ddpwe85529yyv8jc38e8rcmjkxxcx3hx')\n",
    "\n",
    "# Define analysis questions\n",
    "analysis_questions = [\n",
    "    \"Does the paper talk about a medical diseases? If so which?\", \n",
    "    \"What is the paper doing to tackle the disease?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Paper: osteophyte\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided paper, here are the answers to the questions:\n",
      "\n",
      "1. Does the paper talk about a medical disease? If so, which?\n",
      "\n",
      "Yes, the paper talks about a medical condition called Osteoarthritis (OA) and specifically focuses on the detection of osteophytes, which are small bone growths associated with the development and progression of OA.\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "\n",
      "The paper proposes a novel automated patch generation technique called SegPatch, which is used to detect osteophytes in spinal X-ray images. The method combines SegPatch with a fine-tuned DenseNet-121 network to classify patches and identify the presence of osteophytes. The goal is to develop an efficient clinical approach to identifying osteophytes in X-rays, which can help streamline the diagnostic process and improve patient care and treatment outcomes.\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about a medical disease? If so, which?\": \" Yes, the paper talks about a medical condition called Osteoarthritis (OA) and specifically focuses on the detection of osteophytes, which are small bone growths associated with the development and progression of OA.\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" The paper proposes a novel automated patch generation technique called SegPatch, which is used to detect osteophytes in spinal X-ray images. The method combines SegPatch with a fine-tuned DenseNet-121 network to classify patches and identify the presence of osteophytes. The goal is to develop an efficient clinical approach to identifying osteophytes in X-rays, which can help streamline the diagnostic process and improve patient care and treatment outcomes.\",\n",
      "  \"PAPER_TITLE\": \"osteophyte\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: P-ADIC\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided context, I can answer the questions as follows:\n",
      "\n",
      "1. Does the paper talk about a medical disease? If so which?\n",
      "\n",
      "Not available. There is no mention of any medical disease in the provided context. The paper appears to be a mathematical paper discussing p-adic numbers and their properties.\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "\n",
      "Not available. As mentioned earlier, there is no mention of any medical disease in the paper. The paper is focused on mathematical concepts and does not discuss any medical topics.\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about a medical disease? If so which?\": \" Not available. There is no mention of any medical disease in the provided context. The paper appears to be a mathematical paper discussing p-adic numbers and their properties.\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" Not available. As mentioned earlier, there is no mention of any medical disease in the paper. The paper is focused on mathematical concepts and does not discuss any medical topics.\",\n",
      "  \"PAPER_TITLE\": \"P-ADIC\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: Natural_to_mri\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided context, I can answer the following questions:\n",
      "\n",
      "1. Does the paper talk about a medical disease? If so, which?\n",
      "\n",
      "Yes, the paper talks about Alzheimer's disease (AD).\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "\n",
      "The paper is exploring the use of deep learning models, specifically 2D-Slice-CNNs, to analyze MRI scans for brain age prediction and Alzheimer's disease diagnosis. They are investigating the effectiveness of using pre-trained models on natural images (2D) and fine-tuning them for neuroimaging tasks, as well as incorporating positional encodings to preserve spatial information.\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about a medical disease? If so, which?\": \" Yes, the paper talks about Alzheimer's disease (AD).\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" The paper is exploring the use of deep learning models, specifically 2D-Slice-CNNs, to analyze MRI scans for brain age prediction and Alzheimer's disease diagnosis. They are investigating the effectiveness of using pre-trained models on natural images (2D) and fine-tuning them for neuroimaging tasks, as well as incorporating positional encodings to preserve spatial information.\",\n",
      "  \"PAPER_TITLE\": \"Natural_to_mri\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: CT\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided context, here are the answers to your questions:\n",
      "\n",
      "1. Does the paper talk about a medical disease? If so, which?\n",
      "\n",
      "Yes, the paper talks about cystic fibrosis (CF), a genetic disorder that affects the respiratory, digestive, and reproductive systems.\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "\n",
      "The paper is using deep learning algorithms, specifically 2D and 3D Convolutional Neural Networks (CNNs), to segment and identify CF lesions in CT scans. The goal is to improve the accuracy of detecting and quantifying CF-related abnormalities, which can help in monitoring the progression and severity of the disease. The study also explores the use of alternative loss functions to enhance the performance of the segmentation models.\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about a medical disease? If so, which?\": \" Yes, the paper talks about cystic fibrosis (CF), a genetic disorder that affects the respiratory, digestive, and reproductive systems.\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" The paper is using deep learning algorithms, specifically 2D and 3D Convolutional Neural Networks (CNNs), to segment and identify CF lesions in CT scans. The goal is to improve the accuracy of detecting and quantifying CF-related abnormalities, which can help in monitoring the progression and severity of the disease. The study also explores the use of alternative loss functions to enhance the performance of the segmentation models.\",\n",
      "  \"PAPER_TITLE\": \"CT\"\n",
      "}\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'message': 'Please reduce the length of the messages or completion. Current length is 10799 while limit is 8192', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper_title, paper_content \u001b[38;5;129;01min\u001b[39;00m papers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m generate_paper_prompt(paper_title, paper_content, analysis_questions)\n\u001b[0;32m----> 7\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant analyzing research papers.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.1-70b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/backup/soumya/env/ollama/lib/python3.12/site-packages/cerebras/cloud/sdk/resources/chat/completions.py:152\u001b[0m, in \u001b[0;36mCompletionsResource.create\u001b[0;34m(self, messages, model, frequency_penalty, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, x_amz_cf_id, x_delay_time, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chat\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstrip_not_given(\n\u001b[1;32m    143\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    149\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    151\u001b[0m     CompletionCreateResponse,\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCompletionCreateResponse\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Union types cannot be passed in as arguments in the type system\u001b[39;49;00m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletionCreateResponse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    189\u001b[0m )\n",
      "File \u001b[0;32m~/backup/soumya/env/ollama/lib/python3.12/site-packages/cerebras/cloud/sdk/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/backup/soumya/env/ollama/lib/python3.12/site-packages/cerebras/cloud/sdk/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/backup/soumya/env/ollama/lib/python3.12/site-packages/cerebras/cloud/sdk/_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1048\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'message': 'Please reduce the length of the messages or completion. Current length is 10799 while limit is 8192', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}"
     ]
    }
   ],
   "source": [
    "# Process each paper individually\n",
    "paper_analyses = []\n",
    "\n",
    "for paper_title, paper_content in papers.items():\n",
    "    prompt = generate_paper_prompt(paper_title, paper_content, analysis_questions)\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant analyzing research papers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"llama3.1-70b\",\n",
    "        stream=True,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "        top_p=1\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    print('--------------------------------------------------------------')\n",
    "    print(f\"Paper: {paper_title}\")\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('RAW: \\n', response)\n",
    "\n",
    "    analysis = parse_paper_analysis(response)\n",
    "    analysis['PAPER_TITLE'] = paper_title\n",
    "    paper_analyses.append(analysis)\n",
    "    \n",
    "    print()\n",
    "    print('FILTERED')\n",
    "    print(json.dumps(analysis, indent=2))   \n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompt context for second inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_analysis_prompt(paper_analyses):\n",
    "    def safe_get(analysis, key):\n",
    "        # Try different possible keys\n",
    "        possible_keys = [\n",
    "            key,\n",
    "            key.lower(),\n",
    "            key.replace(\"?\", \"\").strip(),\n",
    "            ' '.join(key.split()[1:])  # Remove the number at the start\n",
    "        ]\n",
    "        for possible_key in possible_keys:\n",
    "            if possible_key in analysis:\n",
    "                return analysis[possible_key]\n",
    "        return \"N/A\"  # Return N/A if no matching key is found\n",
    "\n",
    "    papers_info = \"\\n\\n\".join([\n",
    "        f\"Paper: {analysis.get('PAPER_TITLE', 'Untitled')}\\n\"\n",
    "        f\"Disease: {safe_get(analysis, '1. Does the paper talk about a medical disease? If so, which?')}\\n\"\n",
    "        f\"Approach: {safe_get(analysis, '2. What is the paper doing to tackle the disease?')}\"\n",
    "        for analysis in paper_analyses\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Based on the following summaries of research papers, provide a general overview of the medical diseases discussed and the approaches being used to tackle them. Your response should include:\n",
    "\n",
    "1. A comprehensive answer summarizing the diseases mentioned and the various approaches used across all papers.\n",
    "2. A list of the specific papers you considered in your analysis.\n",
    "\n",
    "Here are the paper summaries:\n",
    "\n",
    "{papers_info}\n",
    "\n",
    "Please structure your response as follows:\n",
    "1. Overall Summary: [Your comprehensive answer here]\n",
    "2. Papers Analyzed: [List of paper titles]\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "summary_prompt = generate_summary_analysis_prompt(paper_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 2:\n",
    "\n",
    "Inference on the combined output of Inference 1 for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY ANALYSIS:\n",
      "**Overall Summary**\n",
      "\n",
      "This analysis covers research papers discussing various medical diseases and the approaches being used to tackle them. The diseases mentioned include osteoarthritis (OA), Alzheimer's Disease (AD), and cystic fibrosis (CF). \n",
      "\n",
      "To address these diseases, researchers are leveraging deep learning techniques and Convolutional Neural Networks (CNNs) to analyze medical images such as X-ray images, MRI scans, and CT scans. The approaches being used include:\n",
      "\n",
      "* Automated patch extraction and classification for osteophyte detection in spinal X-ray images (SegPatch and DenseNet-121 network)\n",
      "* Deep learning methods, specifically 2D-Slice-CNN models, for Alzheimer's disease diagnosis and brain age prediction from MRI scans\n",
      "* Convolutional Neural Networks (CNNs) in both 2D and 3D formats for segmenting cystic fibrosis lesions from CT scans\n",
      "\n",
      "These approaches aim to improve diagnosis accuracy, streamline clinical workflows, and ultimately enhance patient care. The use of deep learning techniques and CNNs demonstrates the growing importance of AI in medical research and its potential to revolutionize disease diagnosis and treatment.\n",
      "\n",
      "**Papers Analyzed**\n",
      "\n",
      "1. osteophyte\n",
      "2. Natural_to_mri\n",
      "3. CT\n",
      "\n",
      "Note: The paper \"P-ADIC\" was not included in the analysis as it does not discuss a medical disease.\n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to the Cerebras model\n",
    "stream = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant analyzing research papers.\"},\n",
    "        {\"role\": \"user\", \"content\": summary_prompt}\n",
    "    ],\n",
    "    model=\"llama3.1-70b\",\n",
    "    stream=True,\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "# Collect the response\n",
    "summary_response = \"\"\n",
    "for chunk in stream:\n",
    "    summary_response += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "print('SUMMARY ANALYSIS:')\n",
    "print(summary_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompt context for third inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_followup_questions(client, paper_analyses, summary_response):\n",
    "\n",
    "    context = f\"\"\"\n",
    "Summary of Research Papers:\n",
    "{summary_response}\n",
    "\n",
    "Detailed Paper Analyses:\n",
    "{json.dumps(paper_analyses, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\nYou can now ask follow-up questions about the research papers. Type 'exit' to end the session.\")\n",
    "\n",
    "    while True:\n",
    "        user_question = input(\"\\nYour follow-up question: \")\n",
    "        \n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Ending the follow-up session. Thank you!\")\n",
    "            break\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Based on the following context about several research papers, please answer the user's question:\n",
    "\n",
    "{context}\n",
    "\n",
    "User's question: {user_question}\n",
    "\n",
    "Please provide a concise and accurate answer based only on the information given in the context. If the question cannot be answered using the provided information, please respond with \"I'm sorry, but I don't have enough information to answer that question based on the given context.\"\n",
    "\"\"\"\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant analyzing research papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"llama3.1-70b\",\n",
    "            stream=True,\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "            top_p=1\n",
    "        )\n",
    "\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "        print(\"\\nAnswer:\", response.strip())\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 3:\n",
    "\n",
    "Asking for a follow_up_response based on everything above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can now ask follow-up questions about the research papers. Type 'exit' to end the session.\n",
      "\n",
      "Answer: I'm sorry, but I don't have enough information to answer that question based on the given context.\n",
      "\n",
      "The provided context discusses the application of deep learning techniques and Convolutional Neural Networks (CNNs) in medical research, specifically in the diagnosis and treatment of osteoarthritis, Alzheimer's Disease, and cystic fibrosis. However, it does not provide a general evaluation or assessment of AI as a whole, nor does it discuss the broader implications or value of AI beyond the specific medical applications mentioned.\n"
     ]
    }
   ],
   "source": [
    "# Start the interactive follow-up session\n",
    "follow_up_response = handle_followup_questions(client, paper_analyses, summary_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
