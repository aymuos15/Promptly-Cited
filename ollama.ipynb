{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "# from cerebras.cloud.sdk import Cerebras\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tex(text):\n",
    "    # Basic preprocessing to remove LaTeX commands, but keep section headers\n",
    "    text = re.sub(r'\\\\(?!section|subsection|subsubsection)([a-zA-Z]+)(\\[.*?\\])?({.*?})?', '', text)\n",
    "    return text\n",
    "\n",
    "#? getting all the tex files from every subdirectory\n",
    "def get_all_tex_files(directory):\n",
    "    tex_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.tex'):\n",
    "                tex_files.append(os.path.join(root, file))\n",
    "    return tex_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompt context for first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'dataset' #? Where all you extracted zip of every paper is.\n",
    "tex_files = get_all_tex_files(base_dir) #? Getting all the tex files from the directory\n",
    "\n",
    "# Process files\n",
    "papers = {}\n",
    "for file in tex_files: # Going through one folder at a time\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "    preprocessed_content = preprocess_tex(content) # Gets rid of all the latex commands\n",
    "\n",
    "    # remove 1000 characters from the end of the content and the start of the content\n",
    "    preprocessed_content = preprocessed_content[1000:-1000]\n",
    "\n",
    "    paper_name = os.path.relpath(file, base_dir).split(os.sep)[0] #? Getting the name of the paper from the folder name\n",
    "\n",
    "    # paper name here actually has all the subdirectories as well, so we remove them by omitting all which do not have the exact paper name.    \n",
    "    if paper_name not in papers:\n",
    "        papers[paper_name] = \"\"\n",
    "    papers[paper_name] += preprocessed_content + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paper_prompt(paper_title, paper_content, analysis_questions):\n",
    "    questions_str = \"\\n\".join(f\"- {q}\" for q in analysis_questions)\n",
    "    return f\"\"\"\n",
    "======== START OF PAPER: {paper_title} ========\n",
    "\n",
    "{paper_content}\n",
    "\n",
    "======== END OF PAPER: {paper_title} ========\n",
    "\n",
    "Please analyze this paper and answer the following questions:\n",
    "\n",
    "{questions_str}\n",
    "\n",
    "Base your analysis ONLY on the provided context. If you can't find information for a question, respond with \"Not available\".\n",
    "\"\"\"\n",
    "\n",
    "def parse_paper_analysis(response):\n",
    "    lines = response.split('\\n')\n",
    "    analysis = {}\n",
    "    current_key = None\n",
    "    for line in lines:\n",
    "        if line.strip().startswith('1.') or line.strip().startswith('2.'):\n",
    "            current_key = line.strip()\n",
    "            analysis[current_key] = \"\"\n",
    "        elif current_key and line.strip():\n",
    "            analysis[current_key] += ' ' + line.strip()\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 1:\n",
    "\n",
    "Inference on each paper indiviually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Cerebras client\n",
    "# client = Cerebras(api_key='csk-83cjr2kjp5dke2f2ddpwe85529yyv8jc38e8rcmjkxxcx3hx')\n",
    "\n",
    "# Define analysis questions\n",
    "analysis_questions = [\n",
    "    \"Does the paper talk about a medical diseases? If so which?\", \n",
    "    \"What is the paper doing to tackle the disease?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Paper: osteophyte\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided context, here are the answers to your questions:\n",
      "\n",
      "1. Does the paper talk about a medical diseases? If so which?\n",
      "The paper talks about spinal osteophyte detection.\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "\n",
      "The paper presents a study for spinal osteophyte detection using a specialized patching strategy called SegPatch in conjunction with a fine-tuned DenseNet-121 network. The authors also investigate the use of off-the-shelf detectors, specifically FasterRCNN, but find poor performance. The paper concludes that a more efficient clinical approach to identifying osteophytes can be achieved through the proposed method, which achieves an accuracy score of 84.5% and a specificity score of 86.62%.\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about a medical diseases? If so which?\": \" The paper talks about spinal osteophyte detection.\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" The paper presents a study for spinal osteophyte detection using a specialized patching strategy called SegPatch in conjunction with a fine-tuned DenseNet-121 network. The authors also investigate the use of off-the-shelf detectors, specifically FasterRCNN, but find poor performance. The paper concludes that a more efficient clinical approach to identifying osteophytes can be achieved through the proposed method, which achieves an accuracy score of 84.5% and a specificity score of 86.62%.\",\n",
      "  \"PAPER_TITLE\": \"osteophyte\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: P-ADIC\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided context, here are my answers:\n",
      "\n",
      "* Does the paper talk about a medical disease? If so, which?\n",
      "\t+ No, the paper does not discuss any medical diseases.\n",
      "* What is the paper doing to tackle the disease?\n",
      "\t+ Not applicable (since there is no discussion of a medical disease).\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"PAPER_TITLE\": \"P-ADIC\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: Natural_to_mri\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided paper, here are my answers:\n",
      "\n",
      "1. Does the paper talk about medical diseases? If so, which?\n",
      "The paper mentions Alzheimer's Disease (AD) as one of the disease it is studying.\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "\n",
      "The paper is exploring ways to improve the performance of 3D MRI scans in predicting AD and brain age using 2D-Slice-CNN models with slice encoders pre-trained on natural images (ImageNet). Specifically, the authors are:\n",
      "\n",
      "* Studying the effect of replacing the slice encoder with a model pre-trained on ImageNet\n",
      "* Investigating the impact of incorporating positional encoding in the 2D-Slice-CNN model before slice embedding aggregation to preserve spatial information\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about medical diseases? If so, which?\": \" The paper mentions Alzheimer's Disease (AD) as one of the disease it is studying.\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" The paper is exploring ways to improve the performance of 3D MRI scans in predicting AD and brain age using 2D-Slice-CNN models with slice encoders pre-trained on natural images (ImageNet). Specifically, the authors are: * Studying the effect of replacing the slice encoder with a model pre-trained on ImageNet * Investigating the impact of incorporating positional encoding in the 2D-Slice-CNN model before slice embedding aggregation to preserve spatial information\",\n",
      "  \"PAPER_TITLE\": \"Natural_to_mri\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: CT\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided text, here are my answers:\n",
      "\n",
      "1. Does the paper talk about a medical diseases? If so which?\n",
      "Yes, the paper talks about Chronic Fibrosis (CF). Specifically, it discusses segmenting bronchiectasis and peribronchial thickening, as well as detecting mucus and consolidations.\n",
      "\n",
      "2. What is the paper doing to tackle the disease?\n",
      "The paper is using deep learning models, specifically nnU-Net, to develop a computer-aided diagnosis (CAD) system for identifying CF lesions in computed tomography (CT) scans. The authors compare the performance of 2D and 3D models with different loss functions to determine which approach is most effective in detecting various types of lesions. The goal is to improve the accuracy of disease detection and provide better patient outcomes.\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"1. Does the paper talk about a medical diseases? If so which?\": \" Yes, the paper talks about Chronic Fibrosis (CF). Specifically, it discusses segmenting bronchiectasis and peribronchial thickening, as well as detecting mucus and consolidations.\",\n",
      "  \"2. What is the paper doing to tackle the disease?\": \" The paper is using deep learning models, specifically nnU-Net, to develop a computer-aided diagnosis (CAD) system for identifying CF lesions in computed tomography (CT) scans. The authors compare the performance of 2D and 3D models with different loss functions to determine which approach is most effective in detecting various types of lesions. The goal is to improve the accuracy of disease detection and provide better patient outcomes.\",\n",
      "  \"PAPER_TITLE\": \"CT\"\n",
      "}\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Paper: supply-chain\n",
      "--------------------------------------------------------------\n",
      "RAW: \n",
      " Based on the provided context, here are my answers to your questions:\n",
      "\n",
      "* Does the paper talk about a medical disease? If so which?\n",
      "\t+ No, the paper does not discuss any medical diseases.\n",
      "* What is the paper doing to tackle the disease?\n",
      "\t+ Not available (since there is no discussion of medical diseases)\n",
      "\n",
      "FILTERED\n",
      "{\n",
      "  \"PAPER_TITLE\": \"supply-chain\"\n",
      "}\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process each paper individually\n",
    "paper_analyses = []\n",
    "\n",
    "for paper_title, paper_content in papers.items():\n",
    "    prompt = generate_paper_prompt(paper_title, paper_content, analysis_questions)\n",
    "\n",
    "    # Ollama generate request\n",
    "    response = ollama.generate(\n",
    "        model='llama3',  # Change this to 'llama3' if you have that model available\n",
    "        prompt=prompt,\n",
    "        system=\"You are a helpful assistant analyzing research papers.\",\n",
    "        options={\n",
    "            \"num_predict\": 2000,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 1\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    generated_text = response['response']\n",
    "    \n",
    "    print('--------------------------------------------------------------')\n",
    "    print(f\"Paper: {paper_title}\")\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('RAW: \\n', generated_text)\n",
    "\n",
    "    analysis = parse_paper_analysis(generated_text)\n",
    "    analysis['PAPER_TITLE'] = paper_title\n",
    "    paper_analyses.append(analysis)\n",
    "    \n",
    "    print()\n",
    "    print('FILTERED')\n",
    "    print(json.dumps(analysis, indent=2))   \n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompt context for second inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_analysis_prompt(paper_analyses):\n",
    "    def safe_get(analysis, key):\n",
    "        # Try different possible keys\n",
    "        possible_keys = [\n",
    "            key,\n",
    "            key.lower(),\n",
    "            key.replace(\"?\", \"\").strip(),\n",
    "            ' '.join(key.split()[1:])  # Remove the number at the start\n",
    "        ]\n",
    "        for possible_key in possible_keys:\n",
    "            if possible_key in analysis:\n",
    "                return analysis[possible_key]\n",
    "        return \"N/A\"  # Return N/A if no matching key is found\n",
    "\n",
    "    papers_info = \"\\n\\n\".join([\n",
    "        f\"Paper: {analysis.get('PAPER_TITLE', 'Untitled')}\\n\"\n",
    "        f\"Disease: {safe_get(analysis, '1. Does the paper talk about a medical disease? If so, which?')}\\n\"\n",
    "        f\"Approach: {safe_get(analysis, '2. What is the paper doing to tackle the disease?')}\"\n",
    "        for analysis in paper_analyses\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Based on the following summaries of research papers, provide a general overview of the medical diseases discussed and the approaches being used to tackle them. Your response should include:\n",
    "\n",
    "1. A comprehensive answer summarizing the diseases mentioned and the various approaches used across all papers.\n",
    "2. A list of the specific papers you considered in your analysis.\n",
    "\n",
    "Here are the paper summaries:\n",
    "\n",
    "{papers_info}\n",
    "\n",
    "Please structure your response as follows:\n",
    "1. Overall Summary: [Your comprehensive answer here]\n",
    "2. Papers Analyzed: [List of paper titles]\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "summary_prompt = generate_summary_analysis_prompt(paper_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 2:\n",
    "\n",
    "Inference on the combined output of Inference 1 for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY ANALYSIS:\n",
      "**Overall Summary**\n",
      "\n",
      "The medical diseases discussed in these research papers are not explicitly stated, as three out of the four papers do not mention a specific disease. However, the approaches used in these studies focus on developing computer-aided diagnosis (CAD) systems for identifying and detecting various types of lesions or abnormalities.\n",
      "\n",
      "Three papers employ deep learning models to analyze medical images: one uses a DenseNet-121 network for spinal osteophyte detection, another explores the use of 2D-Slice-CNN models with slice encoders pre-trained on natural images for predicting Alzheimer's disease (AD) and brain age from MRI scans, and the third develops a CAD system using nnU-Net to identify cystic fibrosis (CF) lesions in CT scans. These studies aim to improve the accuracy of disease detection and provide better patient outcomes.\n",
      "\n",
      "**Papers Analyzed**\n",
      "\n",
      "1. Osteophyte\n",
      "2. P-ADIC\n",
      "3. Natural_to_mri\n",
      "4. CT\n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to the Ollama model for summary\n",
    "summary_response = ollama.generate(\n",
    "    model='llama3',  # Change this to 'llama3' if you have that model available\n",
    "    prompt=summary_prompt,\n",
    "    system=\"You are a helpful assistant analyzing research papers.\",\n",
    "    options={\n",
    "        \"num_predict\": 2000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "summary_text = summary_response['response']\n",
    "\n",
    "print('SUMMARY ANALYSIS:')\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompt context for third inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_followup_questions(paper_analyses, summary_response):\n",
    "    context = f\"\"\"\n",
    "Summary of Research Papers:\n",
    "{summary_response}\n",
    "\n",
    "Detailed Paper Analyses:\n",
    "{json.dumps(paper_analyses, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\nYou can now ask follow-up questions about the research papers. Type 'exit' to end the session.\")\n",
    "\n",
    "    while True:\n",
    "        user_question = input(\"Your question: \")\n",
    "        \n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Ending the follow-up session. Thank you!\")\n",
    "            break\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Based on the following context about several research papers, please answer the user's question:\n",
    "\n",
    "{context}\n",
    "\n",
    "User's question: {user_question}\n",
    "\n",
    "Please provide a concise and accurate answer based only on the information given in the context. If the question cannot be answered using the provided information, please respond with \"I'm sorry, but I don't have enough information to answer that question based on the given context.\"\n",
    "\"\"\"\n",
    "\n",
    "        response = ollama.generate(\n",
    "            model='llama3',  # Change this to 'llama3' if you have that model available\n",
    "            prompt=prompt,\n",
    "            system=\"You are a helpful assistant analyzing research papers.\",\n",
    "            options={\n",
    "                \"num_predict\": 500,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 1\n",
    "            }\n",
    "        )\n",
    "\n",
    "        answer = response['response']\n",
    "        print(\"\\nAnswer:\", answer.strip())\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 3:\n",
    "\n",
    "Asking for a follow_up_response based on everything above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can now ask follow-up questions about the research papers. Type 'exit' to end the session.\n",
      "\n",
      "Answer: Based on the provided context, there is no mention of corn or any related medical diseases. The papers discussed are about spinal osteophyte detection, Alzheimer's Disease (AD), and Chronic Fibrosis (CF). Therefore, I'm sorry, but I don't have enough information to answer that question based on the given context.\n",
      "\n",
      "Answer: Based on the provided context, it appears that there is a paper titled \"P-ADIC\" which mentions no specific details about p-adic numbers or whether machine learning (ML) is used to tackle any related problems. Therefore, I can only say that there is no information in this context that suggests ML is good for p-adic numbers or if it is even relevant to the topic.\n",
      "\n",
      "Answer: I'm sorry, but I don't have enough information to answer that question based on the given context.\n",
      "\n",
      "Answer: Based on the provided context, there is some evidence of medical disease detection being explored through machine learning (ML) in certain papers. Specifically:\n",
      "\n",
      "* The paper titled \"osteophyte\" uses a specialized patching strategy and fine-tuned DenseNet-121 network to detect spinal osteophytes with an accuracy score of 84.5% and specificity score of 86.62%.\n",
      "* The paper titled \"Natural_to_mri\" explores ways to improve the performance of 3D MRI scans in predicting Alzheimer's Disease (AD) and brain age using 2D-Slice-CNN models.\n",
      "* The paper titled \"CT\" uses deep learning models, specifically nnU-Net, to develop a computer-aided diagnosis (CAD) system for identifying Chronic Fibrosis (CF) lesions in computed tomography (CT) scans.\n",
      "\n",
      "While these examples suggest that ML can be used to detect diseases with varying degrees of success, it is essential to consider the specific challenges and limitations of each disease and the quality of the data being used. Therefore, I would say that, based on this limited information, it is possible to use ML to detect diseases confidently, but more research and consideration are required to ensure accurate and reliable results.\n",
      "\n",
      "Answer: Yes, you can use machine learning to detect diseases, but it's crucial to consider the specific challenges and limitations of each disease and the quality of the data being used.\n",
      "Ending the follow-up session. Thank you!\n"
     ]
    }
   ],
   "source": [
    "# Start the interactive follow-up session\n",
    "follow_up_response = handle_followup_questions(paper_analyses, summary_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
